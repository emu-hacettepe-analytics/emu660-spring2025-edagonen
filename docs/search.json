[
  {
    "objectID": "project.html",
    "href": "project.html",
    "title": "Project Genre Matters: Exploring Film Audience Preferences",
    "section": "",
    "text": "Welcome to our project page.\nBeg√ºm √áORUH & Eda G√ñNEN\nKeep an eye on this space to stay updated with my project activities."
  },
  {
    "objectID": "project.html#data-source",
    "href": "project.html#data-source",
    "title": "Project X",
    "section": "2.1 Data Source",
    "text": "2.1 Data Source\nxxxxxx"
  },
  {
    "objectID": "project.html#general-information-about-data",
    "href": "project.html#general-information-about-data",
    "title": "Project X",
    "section": "2.2 General Information About Data",
    "text": "2.2 General Information About Data\nxxxxxx"
  },
  {
    "objectID": "project.html#reason-of-choice",
    "href": "project.html#reason-of-choice",
    "title": "Project X",
    "section": "2.3 Reason of Choice",
    "text": "2.3 Reason of Choice\nxxxxxx"
  },
  {
    "objectID": "project.html#preprocessing",
    "href": "project.html#preprocessing",
    "title": "Project X",
    "section": "2.4 Preprocessing",
    "text": "2.4 Preprocessing\nxxxxxx"
  },
  {
    "objectID": "project.html#exploratory-data-analysis",
    "href": "project.html#exploratory-data-analysis",
    "title": "Project X",
    "section": "3.1 Exploratory Data Analysis",
    "text": "3.1 Exploratory Data Analysis\nxxxxxx"
  },
  {
    "objectID": "project.html#trend-analysis",
    "href": "project.html#trend-analysis",
    "title": "Project X",
    "section": "3.2 Trend Analysis",
    "text": "3.2 Trend Analysis\nxxxxxx"
  },
  {
    "objectID": "project.html#model-fitting",
    "href": "project.html#model-fitting",
    "title": "Project X",
    "section": "3.3 Model Fitting",
    "text": "3.3 Model Fitting\nxxxxxx"
  },
  {
    "objectID": "project.html#results",
    "href": "project.html#results",
    "title": "Project X",
    "section": "3.4 Results",
    "text": "3.4 Results\nxxxxxx"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Analytic Lab",
    "section": "",
    "text": "Hello! I‚Äôm Eda G√∂nen.\nWelcome to my personal website! Here, I share my work in data analytics, blog posts, and a few different projects. I enjoy working with data and finding new ways to understand it. I‚Äôll also be sharing my thoughts on technology from time to time.\nStay tuned, there‚Äôs plenty to discover and learn together!\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html#deneme",
    "href": "index.html#deneme",
    "title": "Welcome to My Analytics Lab",
    "section": "Deneme",
    "text": "Deneme"
  },
  {
    "objectID": "assignments/statisticsofmrcars.html",
    "href": "assignments/statisticsofmrcars.html",
    "title": "Mtcars Statistical Analysis",
    "section": "",
    "text": "In R, data(mtcars) loads the mtcars dataset, which contains features of 32 cars. str(mtcars) then displays the dataset‚Äôs structure, revealing the types and initial values of its columns, providing a quick overview.\n\ndata(mtcars)\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\n\nThis R code calculates basic statistics for numerical columns in the mtcars dataset. It defines a function, compute_stats, to find the mean, median, variance, IQR, min, and max of a numeric vector, handling potential missing values. A for loop iterates through each column of mtcars, applying compute_stats to numerical ones. The results, with column names, are stored in a list and printed, providing a summary of each numerical column‚Äôs statistics.\n\n# ƒ∞statistikleri hesaplayan fonksiyon\ncompute_stats &lt;- function(x) {\n  # Girdi sayƒ±sal mƒ± kontrol et\n  if (!is.numeric(x)) {\n    stop(\"Hata: Girdi sayƒ±sal bir vekt√∂r olmalƒ±dƒ±r.\")\n  }\n\n  # ƒ∞statistik hesaplamalarƒ±\n  mean_x &lt;- mean(x, na.rm = TRUE)\n  median_x &lt;- median(x, na.rm = TRUE)\n  var_x &lt;- var(x, na.rm = TRUE)\n  iqr_x &lt;- IQR(x, na.rm = TRUE)\n  min_x &lt;- min(x, na.rm = TRUE)\n  max_x &lt;- max(x, na.rm = TRUE)\n\n  # Sonu√ßlarƒ± i√ßeren listeyi d√∂nd√ºr\n  compute_stats_list &lt;- list(\n    mean = mean_x,\n    median = median_x,\n    variance = var_x,\n    IQR = iqr_x,\n    min = min_x,\n    maks = max_x\n  )\n\n  return(compute_stats_list)\n}\n\n# mtcars veri seti i√ßin t√ºm sayƒ±sal s√ºtunlarƒ±n istatistiklerini hesapla\nmtcars_istatistikleri &lt;- list() # Bo≈ü bir liste olu≈ütur\n\nfor (header in names(mtcars)) {\n  if (is.numeric(mtcars[[header]])) {\n    mtcars_istatistikleri[[header]] &lt;- compute_stats(mtcars[[header]]) # Sonu√ßlarƒ± listeye ekle\n  }\n}\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(mtcars_istatistikleri)\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$maks\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$maks\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$maks\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$maks\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$maks\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$maks\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$maks\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$maks\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$maks\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$maks\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$maks\n[1] 8\n\n\nTo automate statistical analysis of numerical columns within the mtcars dataset, a for loop iterates through each column name. Utilizing is.numeric(), the loop identifies numerical columns and applies the compute_stats function. This function calculates key statistics like mean, median, and variance, returning them in a named list. The loop then stores these results, indexed by column names, within a comprehensive list, which is subsequently printed. This approach efficiently provides a structured statistical overview of all numerical columns in the dataset.\n\n\n\nThe sapply function in R is a user-friendly and efficient way to apply a function over a list or vector. In the context of the mtcars dataset, sapply iterates through each column, applying the compute_stats function to those that are numeric. This streamlines the process of calculating statistics for multiple columns, returning the results in a simplified format, such as a vector or matrix. Its ability to directly handle data frames makes it a powerful tool for quick statistical analysis.\n\n# sapply ile t√ºm s√ºtunlara compute_stats fonksiyonunu uygula\nmtcars_istatistikleri_sapply &lt;- sapply(mtcars, compute_stats)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(mtcars_istatistikleri_sapply)\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmaks     33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmaks     1         1         5         8       \n\n\nThe apply function in R is designed to apply a function over the rows or columns of a matrix or array. In this scenario, the mtcars data frame is first converted to a matrix. Then, apply is used to iterate over the columns, applying the compute_stats function to each. This approach allows for consistent application of statistical calculations across all columns, returning the results in a structured format. While apply is versatile, it requires the data to be in a matrix format, making it slightly less direct for data frames compared to sapply.\n\n# apply ile t√ºm s√ºtunlara compute_stats fonksiyonunu uygula\nmtcars_istatistikleri_apply &lt;- apply(mtcars, MARGIN = 2, compute_stats)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(mtcars_istatistikleri_apply)\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$maks\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$maks\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$maks\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$maks\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$maks\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$maks\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$maks\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$maks\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$maks\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$maks\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$maks\n[1] 8",
    "crumbs": [
      "MRCars Statistics"
    ]
  },
  {
    "objectID": "assignments/statisticsofmrcars.html#statistical-analysis-of-the-mtcars-dataset-in-r",
    "href": "assignments/statisticsofmrcars.html#statistical-analysis-of-the-mtcars-dataset-in-r",
    "title": "Mtcars Statistical Analysis",
    "section": "",
    "text": "In R, data(mtcars) loads the mtcars dataset, which contains features of 32 cars. str(mtcars) then displays the dataset‚Äôs structure, revealing the types and initial values of its columns, providing a quick overview.\n\ndata(mtcars)\nstr(mtcars)\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\n\n\n\n\nThis R code calculates basic statistics for numerical columns in the mtcars dataset. It defines a function, compute_stats, to find the mean, median, variance, IQR, min, and max of a numeric vector, handling potential missing values. A for loop iterates through each column of mtcars, applying compute_stats to numerical ones. The results, with column names, are stored in a list and printed, providing a summary of each numerical column‚Äôs statistics.\n\n# ƒ∞statistikleri hesaplayan fonksiyon\ncompute_stats &lt;- function(x) {\n  # Girdi sayƒ±sal mƒ± kontrol et\n  if (!is.numeric(x)) {\n    stop(\"Hata: Girdi sayƒ±sal bir vekt√∂r olmalƒ±dƒ±r.\")\n  }\n\n  # ƒ∞statistik hesaplamalarƒ±\n  mean_x &lt;- mean(x, na.rm = TRUE)\n  median_x &lt;- median(x, na.rm = TRUE)\n  var_x &lt;- var(x, na.rm = TRUE)\n  iqr_x &lt;- IQR(x, na.rm = TRUE)\n  min_x &lt;- min(x, na.rm = TRUE)\n  max_x &lt;- max(x, na.rm = TRUE)\n\n  # Sonu√ßlarƒ± i√ßeren listeyi d√∂nd√ºr\n  compute_stats_list &lt;- list(\n    mean = mean_x,\n    median = median_x,\n    variance = var_x,\n    IQR = iqr_x,\n    min = min_x,\n    maks = max_x\n  )\n\n  return(compute_stats_list)\n}\n\n# mtcars veri seti i√ßin t√ºm sayƒ±sal s√ºtunlarƒ±n istatistiklerini hesapla\nmtcars_istatistikleri &lt;- list() # Bo≈ü bir liste olu≈ütur\n\nfor (header in names(mtcars)) {\n  if (is.numeric(mtcars[[header]])) {\n    mtcars_istatistikleri[[header]] &lt;- compute_stats(mtcars[[header]]) # Sonu√ßlarƒ± listeye ekle\n  }\n}\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(mtcars_istatistikleri)\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$maks\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$maks\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$maks\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$maks\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$maks\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$maks\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$maks\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$maks\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$maks\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$maks\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$maks\n[1] 8\n\n\nTo automate statistical analysis of numerical columns within the mtcars dataset, a for loop iterates through each column name. Utilizing is.numeric(), the loop identifies numerical columns and applies the compute_stats function. This function calculates key statistics like mean, median, and variance, returning them in a named list. The loop then stores these results, indexed by column names, within a comprehensive list, which is subsequently printed. This approach efficiently provides a structured statistical overview of all numerical columns in the dataset.\n\n\n\nThe sapply function in R is a user-friendly and efficient way to apply a function over a list or vector. In the context of the mtcars dataset, sapply iterates through each column, applying the compute_stats function to those that are numeric. This streamlines the process of calculating statistics for multiple columns, returning the results in a simplified format, such as a vector or matrix. Its ability to directly handle data frames makes it a powerful tool for quick statistical analysis.\n\n# sapply ile t√ºm s√ºtunlara compute_stats fonksiyonunu uygula\nmtcars_istatistikleri_sapply &lt;- sapply(mtcars, compute_stats)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(mtcars_istatistikleri_sapply)\n\n         mpg      cyl      disp     hp       drat      wt       qsec    \nmean     20.09062 6.1875   230.7219 146.6875 3.596563  3.21725  17.84875\nmedian   19.2     6        196.3    123      3.695     3.325    17.71   \nvariance 36.3241  3.189516 15360.8  4700.867 0.2858814 0.957379 3.193166\nIQR      7.375    4        205.175  83.5     0.84      1.02875  2.0075  \nmin      10.4     4        71.1     52       2.76      1.513    14.5    \nmaks     33.9     8        472      335      4.93      5.424    22.9    \n         vs        am        gear      carb    \nmean     0.4375    0.40625   3.6875    2.8125  \nmedian   0         0         4         2       \nvariance 0.2540323 0.2489919 0.5443548 2.608871\nIQR      1         1         1         2       \nmin      0         0         3         1       \nmaks     1         1         5         8       \n\n\nThe apply function in R is designed to apply a function over the rows or columns of a matrix or array. In this scenario, the mtcars data frame is first converted to a matrix. Then, apply is used to iterate over the columns, applying the compute_stats function to each. This approach allows for consistent application of statistical calculations across all columns, returning the results in a structured format. While apply is versatile, it requires the data to be in a matrix format, making it slightly less direct for data frames compared to sapply.\n\n# apply ile t√ºm s√ºtunlara compute_stats fonksiyonunu uygula\nmtcars_istatistikleri_apply &lt;- apply(mtcars, MARGIN = 2, compute_stats)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(mtcars_istatistikleri_apply)\n\n$mpg\n$mpg$mean\n[1] 20.09062\n\n$mpg$median\n[1] 19.2\n\n$mpg$variance\n[1] 36.3241\n\n$mpg$IQR\n[1] 7.375\n\n$mpg$min\n[1] 10.4\n\n$mpg$maks\n[1] 33.9\n\n\n$cyl\n$cyl$mean\n[1] 6.1875\n\n$cyl$median\n[1] 6\n\n$cyl$variance\n[1] 3.189516\n\n$cyl$IQR\n[1] 4\n\n$cyl$min\n[1] 4\n\n$cyl$maks\n[1] 8\n\n\n$disp\n$disp$mean\n[1] 230.7219\n\n$disp$median\n[1] 196.3\n\n$disp$variance\n[1] 15360.8\n\n$disp$IQR\n[1] 205.175\n\n$disp$min\n[1] 71.1\n\n$disp$maks\n[1] 472\n\n\n$hp\n$hp$mean\n[1] 146.6875\n\n$hp$median\n[1] 123\n\n$hp$variance\n[1] 4700.867\n\n$hp$IQR\n[1] 83.5\n\n$hp$min\n[1] 52\n\n$hp$maks\n[1] 335\n\n\n$drat\n$drat$mean\n[1] 3.596563\n\n$drat$median\n[1] 3.695\n\n$drat$variance\n[1] 0.2858814\n\n$drat$IQR\n[1] 0.84\n\n$drat$min\n[1] 2.76\n\n$drat$maks\n[1] 4.93\n\n\n$wt\n$wt$mean\n[1] 3.21725\n\n$wt$median\n[1] 3.325\n\n$wt$variance\n[1] 0.957379\n\n$wt$IQR\n[1] 1.02875\n\n$wt$min\n[1] 1.513\n\n$wt$maks\n[1] 5.424\n\n\n$qsec\n$qsec$mean\n[1] 17.84875\n\n$qsec$median\n[1] 17.71\n\n$qsec$variance\n[1] 3.193166\n\n$qsec$IQR\n[1] 2.0075\n\n$qsec$min\n[1] 14.5\n\n$qsec$maks\n[1] 22.9\n\n\n$vs\n$vs$mean\n[1] 0.4375\n\n$vs$median\n[1] 0\n\n$vs$variance\n[1] 0.2540323\n\n$vs$IQR\n[1] 1\n\n$vs$min\n[1] 0\n\n$vs$maks\n[1] 1\n\n\n$am\n$am$mean\n[1] 0.40625\n\n$am$median\n[1] 0\n\n$am$variance\n[1] 0.2489919\n\n$am$IQR\n[1] 1\n\n$am$min\n[1] 0\n\n$am$maks\n[1] 1\n\n\n$gear\n$gear$mean\n[1] 3.6875\n\n$gear$median\n[1] 4\n\n$gear$variance\n[1] 0.5443548\n\n$gear$IQR\n[1] 1\n\n$gear$min\n[1] 3\n\n$gear$maks\n[1] 5\n\n\n$carb\n$carb$mean\n[1] 2.8125\n\n$carb$median\n[1] 2\n\n$carb$variance\n[1] 2.608871\n\n$carb$IQR\n[1] 2\n\n$carb$min\n[1] 1\n\n$carb$maks\n[1] 8",
    "crumbs": [
      "MRCars Statistics"
    ]
  },
  {
    "objectID": "assignments/statisticsofmrcars.html#handling-missing-values-in-the-dslabs-na_example-dataset-methods-and-comparisons",
    "href": "assignments/statisticsofmrcars.html#handling-missing-values-in-the-dslabs-na_example-dataset-methods-and-comparisons",
    "title": "Mtcars Statistical Analysis",
    "section": "Handling Missing Values in the dslabs ‚Äòna_example‚Äô Dataset: Methods and Comparisons",
    "text": "Handling Missing Values in the dslabs ‚Äòna_example‚Äô Dataset: Methods and Comparisons\n\nInstalling and Loading the ‚Äòdslaps‚Äô\n\n#install.packages(\"dslabs\")\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\n\nAfter successfully installing and loading the ‚Äòdslabs‚Äô library, we can now print the desired dataset.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\n\n\nNumber and Locations of NA Values\nThis R code calculates the missing values (NA) within the ‚Äòna_example‚Äô dataset. Initially, we determined the total count of NA values present and identified their index positions within the dataset.\n\ntotal_na &lt;- sum(is.na(na_example))\ncat(\"Total NA values:\", total_na)\n\nTotal NA values: 145\n\n\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\n\n\nStatistical Calculation Ignoring NA Values\n\n# NA deƒüerleri g√∂z ardƒ± ederek ortalama ve standart sapma hesapla\nmean_value &lt;- mean(na_example, na.rm = TRUE)\nsd_value &lt;- sd(na_example, na.rm = TRUE)\n\n# Sonu√ßlarƒ± ekrana yazdƒ±r\ncat(\"Mean:\", mean_value, \"\\n\")\n\nMean: 2.301754 \n\ncat(\"Standart Deviation:\", sd_value)\n\nStandart Deviation: 1.22338\n\n\n\n\nReplacing NA Values with the Median\nIn this process, we replace all missing (NA) values in the na_example dataset with the median of the non-missing values. The median is chosen because it is less sensitive to extreme values (outliers) compared to the mean.\n\n# NA olmayan deƒüerlerin medyanƒ±nƒ± hesapla\nmedian_others &lt;- median(na_example, na.rm = TRUE)\n\n# NA deƒüerleri medyan ile deƒüi≈ütir\nna_example_median &lt;- ifelse(is.na(na_example), median_others, na_example)\n\n# Sonucu yazdƒ±r\ncat(\"Medyan:\", median_others, \"\\n\")\n\nMedyan: 2 \n\nprint(na_example_median)\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\n\nversion1_median &lt;- median(na_example_median)\nversion1_sd &lt;- sd(na_example_median)\n\n# Sonu√ßlarƒ± yazdƒ±r\ncat(\"Versiyon 1 Medyan:\", version1_median, \"\\n\")\n\nVersiyon 1 Medyan: 2 \n\ncat(\"Versiyon 1 Sapma:\", version1_sd)\n\nVersiyon 1 Sapma: 1.136102\n\n\n\n\nReplacing NA Values with Randomly Selected Non-missing Value\nThis process replaces all NA values in the dataset with a randomly selected non-missing value from the same dataset. First, we extract all non-missing values, then for each NA, a random value from the non-missing values is chosen to fill in the missing spot.\n\n# NA olmayan deƒüerleri al\nnon_na_values &lt;- na_example[!is.na(na_example)]\n\n# NA deƒüerlerini rastgele bir NA olmayan deƒüerle deƒüi≈ütir\nna_example_random &lt;- ifelse(is.na(na_example), sample(non_na_values, 1), na_example)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(na_example_random)\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 3 2 2 1 4 3 1 1 2 1 2 2 1 2 5 3 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 3 3 3 1 1 5 1 3 1 3 4 4 7 3 2 3 3 1 3 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 3 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 3 4 3 4 3 1 2 1 3 3 3 3 1 5 1 2 1 3 5 3 2 2 3 3 3 3 3 5 3 1 1 4\n [149] 2 4 3 3 3 2 3 2 6 3 1 1 2 2 1 3 1 1 5 3 3 2 4 3 2 5 1 4 3 3 3 4 3 1 4 1 1\n [186] 3 1 1 3 3 3 5 2 2 2 3 1 2 2 3 2 1 3 2 3 1 3 3 2 1 1 3 3 3 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 3 5 1 4 3 3 3 3 1 1 5 2 3 3 2 4 3 3 2 5 3 2 3\n [260] 4 6 2 2 2 3 2 3 2 3 3 3 2 2 4 3 1 4 2 3 2 4 3 6 2 3 1 3 2 2 3 1 1 3 2 3 3\n [297] 1 3 1 4 2 1 1 3 2 1 2 3 1 3 2 3 3 2 1 2 3 5 5 1 2 3 3 1 3 3 1 2 4 3 2 1 1\n [334] 1 3 2 1 1 3 4 3 1 2 1 1 3 3 3 1 1 3 5 3 2 3 4 1 4 3 1 3 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 3 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 3 3 3 3 2 3 1 2 1 1 4 2 1 4 4 3\n [408] 1 2 3 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 3 4 4 4 1 1 3 4\n [445] 3 3 1 3 1 3 2 4 2 2 2 3 2 1 4 3 3 1 4 3 1 3 2 3 3 3 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 3 3 2 1 1 2 3 2 2 2 3 3 1 1 2 3 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 3 1 4 1 2 4 1 3 2 3 1 1 3 2 1 1 4 2 3 3 1 5 3 1 1 2 3 1 1\n [556] 3 1 3 2 4 3 2 3 2 1 2 1 1 1 2 2 3 1 5 2 3 2 3 3 2 2 2 1 5 3 2 3 1 3 3 1 2\n [593] 2 2 1 2 2 4 3 6 1 2 3 1 1 2 2 3 3 3 2 3 3 4 2 3 2 3 4 3 1 1 2 2 3 1 1 1 3\n [630] 3 2 5 3 7 1 3 4 3 3 1 3 1 1 1 1 3 2 4 2 2 3 3 3 1 4 3 2 2 2 3 2 4 2 2 4 3\n [667] 3 3 6 3 3 1 4 4 2 1 3 1 6 3 3 3 2 1 1 6 3 1 5 1 3 2 6 2 3 4 1 3 1 2 3 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 3 1 2 2 2 2 4 5 3 3 3 4 3 3 3\n [741] 2 4 2 4 3 3 3 3 2 1 3 2 4 3 2 3 2 3 1 3 4 3 1 2 1 2 3 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 3 3 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 3 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 3 1 3 1 2 3 1 2 1 2 1 3 1 3 2 3 2 3 2 1 4 2 3 3 3 2 4 2 3 3 3\n [852] 1 3 5 5 2 2 2 3 2 1 3 1 3 2 4 2 4 3 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 3 3\n [889] 3 2 2 3 3 3 2 1 2 4 1 1 1 1 4 3 2 3 3 2 3 1 3 3 2 1 1 1 2 3 2 2 3 3 2 3 3\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 3 1 1 1 3 4 3 5 1 1 2 3 2 2 2 2 5 2 2 3 1 2 3 3\n [963] 1 2 3 3 2 3 3 1 1 2 5 3 5 1 1 4 3 2 1 3 1 1 2 4 3 3 3 3 1 1 2 2 1 1 2 2 3\n[1000] 2\n\n\n\nversion2_median &lt;- median(na_example_random)\nversion2_sd &lt;- sd(na_example_random)\n\n# Sonu√ßlarƒ± yazdƒ±r\ncat(\"Versiyon 2 Medyan:\", version2_median, \"\\n\")\n\nVersiyon 2 Medyan: 2 \n\ncat(\"Versiyon 2 Sapma:\", version2_sd)\n\nVersiyon 2 Sapma: 1.157554\n\n\n\nlibrary(knitr)  # kable fonksiyonu i√ßin\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\n# Bu kod bloƒüu tamamen AI tarafƒ±ndan yazƒ±lmƒ±stƒ±r.\n# Sonu√ßlarƒ± birle≈ütirip tabloyu olu≈ütur\n\nstatistics_table &lt;- data.frame(\n  Method = c(\"Original (Mean)\", \"Original (SD)\", \n             \"Imputed (Median, Mean)\", \"Imputed (Median, SD)\", \n             \"Imputed (Random, Mean)\", \"Imputed (Random, SD)\"),\n  Value = c(mean_value, sd_value,\n            mean(na_example_median), sd(na_example_median),\n            mean(na_example_random), sd(na_example_random))\n)\n\n# Tabloyu yazdƒ±r\nkable(statistics_table, caption = \"Comparison of Statistics Before and After Handling NA Values\")\n\n\nComparison of Statistics Before and After Handling NA Values\n\n\nMethod\nValue\n\n\n\n\nOriginal (Mean)\n2.301754\n\n\nOriginal (SD)\n1.223380\n\n\nImputed (Median, Mean)\n2.258000\n\n\nImputed (Median, SD)\n1.136102\n\n\nImputed (Random, Mean)\n2.403000\n\n\nImputed (Random, SD)\n1.157554\n\n\n\n\n\nRegarding the data, imputing with the median seems more appropriate as it preserves the central tendency without being affected by outliers. Imputing with random values could better reflect the distribution of the data, but it may introduce some variability. Looking at the original data, if the number of missing values is minimal, ignoring them could be acceptable. However, if the missing values are not random, it could introduce bias in the data.",
    "crumbs": [
      "MRCars Statistics"
    ]
  },
  {
    "objectID": "assignments/datascience.html",
    "href": "assignments/datascience.html",
    "title": "On Data Science and Industrial Engineering",
    "section": "",
    "text": "Below, you will find a brief summary of the discussion video on data analytics and industrial engineering, available at the following link:\nWatch the video here",
    "crumbs": [
      "On Data Science and Industrial Engineering"
    ]
  },
  {
    "objectID": "assignments/datascience.html#the-role-of-data-science-and-its-applications-insights-from-kerem-demirta≈ü",
    "href": "assignments/datascience.html#the-role-of-data-science-and-its-applications-insights-from-kerem-demirta≈ü",
    "title": "On Data Science and Industrial Engineering",
    "section": "The Role of Data Science and Its Applications: Insights from Kerem Demirta≈ü",
    "text": "The Role of Data Science and Its Applications: Insights from Kerem Demirta≈ü\nKerem Demirta≈ü is a Data Scientist currently working at Invent Analytics. Previously, he worked at Spyke Games and Smart Kiwi. One of his significant projects was Royal Reachers, where he focused on analyzing users‚Äô gaming behaviors and demographic structures to deliver the most suitable offers at the most optimal times, thereby maximizing sales. Later in his career, he contributed to a project aimed at optimizing inventory management in retail. This involved developing software that determines how much stock should be held at various locations and for which models, ultimately helping businesses maximize their sales.\nHis research interests revolve around autonomous vehicles, traffic flow modeling, transportation simulations, and optimization techniques in mobility systems.",
    "crumbs": [
      "On Data Science and Industrial Engineering"
    ]
  },
  {
    "objectID": "assignments/datascience.html#the-scope-of-data-science",
    "href": "assignments/datascience.html#the-scope-of-data-science",
    "title": "On Data Science and Industrial Engineering",
    "section": "The Scope of Data Science",
    "text": "The Scope of Data Science\nData science finds applications in various fields, including:\n\nEpidemiology: Predicting the likelihood of disease outbreaks, identifying high-risk regions, and implementing preventive measures.\nRetail: Optimizing stock levels and sales strategies to enhance profitability.\nAutonomous Vehicles: Improving traffic efficiency, reducing congestion, and enhancing the decision-making capabilities of self-driving cars.",
    "crumbs": [
      "On Data Science and Industrial Engineering"
    ]
  },
  {
    "objectID": "assignments/datascience.html#what-data-science-is-not",
    "href": "assignments/datascience.html#what-data-science-is-not",
    "title": "On Data Science and Industrial Engineering",
    "section": "What Data Science is NOT",
    "text": "What Data Science is NOT\n\nIt is not merely an exaggerated form of statistics.\nIt is not just about building models.\nIt is not exclusively tied to Big Data.\nWriting Python code does not make someone a data analyst.\nData analysis does not always provide definitive answers to every question.",
    "crumbs": [
      "On Data Science and Industrial Engineering"
    ]
  },
  {
    "objectID": "assignments/datascience.html#the-data-science-workflow",
    "href": "assignments/datascience.html#the-data-science-workflow",
    "title": "On Data Science and Industrial Engineering",
    "section": "The Data Science Workflow",
    "text": "The Data Science Workflow\n\nProblem Identification: Clearly defining the problem is the first step in data science. A historical example of this is the analysis conducted during World War II on aircraft durability. Engineers initially focused on reinforcing the most frequently damaged parts of returning planes, but statistician Abraham Wald proposed a different approach‚Äîsuggesting that undamaged areas of returning planes were actually where the planes that were shot down had suffered critical damage. This shift in thinking led to more effective reinforcement strategies.\nData Collection: Gathering unbiased data is crucial. Abraham Wald‚Äôs unbiased thinking methodology is a key example. His research showed that survivorship bias can mislead analyses if data from unsuccessful cases (e.g., planes that didn‚Äôt return) is not considered.\nExploratory Data Analysis (EDA): Before building models, it is essential to understand and visualize the data. A classic example is John Snow‚Äôs study on cholera outbreaks in 1854 London. By mapping out cases and identifying contaminated water sources, he demonstrated that cholera was waterborne‚Äîlong before germ theory was widely accepted. This showcases how data visualization and pattern recognition can lead to groundbreaking discoveries.\nModel Building: At this stage, predictive and analytical models are developed. A relevant example is Lewis Fry Richardson‚Äôs pioneering work on weather forecasting, where he proposed numerical methods for predicting atmospheric conditions. Another example is the World War II Diet Problem, which used linear programming to optimize military rations by balancing nutrition and cost.\nModel Evaluation: Assessing model performance and avoiding overfitting are crucial aspects of data science. Overfitting occurs when a model learns noise instead of actual patterns, reducing its real-world applicability. An illustrative case is the Deep Blue vs.¬†Garry Kasparov chess match, where the AI was trained to analyze millions of moves but had to generalize its strategies to defeat a world champion.\nProduction and Live Performance: Once a model is finalized, it must be deployed in a real-world setting. This step involves monitoring AI tools and ensuring their effectiveness. Examples include AI-driven recommendation systems in e-commerce, self-learning fraud detection systems in banking, and real-time traffic prediction tools in transportation networks.",
    "crumbs": [
      "On Data Science and Industrial Engineering"
    ]
  },
  {
    "objectID": "assignments/datascience.html#kerem-demirta≈üs-research-on-autonomous-vehicles",
    "href": "assignments/datascience.html#kerem-demirta≈üs-research-on-autonomous-vehicles",
    "title": "On Data Science and Industrial Engineering",
    "section": "Kerem Demirta≈ü‚Äôs Research on Autonomous Vehicles",
    "text": "Kerem Demirta≈ü‚Äôs Research on Autonomous Vehicles\nHis thesis, ‚ÄúObject-driven Cellular Automaton Model for Platooning of Autonomous Vehicles on Freeways with Multiple Lanes‚Äù, explores how autonomous vehicles can form convoys and adapt to complex road conditions. The primary motivation behind his research is to minimize the following distance between vehicles, optimize acceleration and deceleration during lane changes, and reduce the impact of stop-and-go waves in traffic flow. By refining the mechanics of platooning, his work contributes to the development of more efficient and safer autonomous driving systems.\nUltimately, his work highlights the power of data science in solving complex real-world problems, from optimizing business operations to transforming the future of mobility.",
    "crumbs": [
      "On Data Science and Industrial Engineering"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "As an industrial engineer with a passion for data analysis, broadcasting, and artificial intelligence, I have built my career on exploring innovative solutions in these fields. After graduating from TOBB University, I began working as a data analyst in the broadcasting sector, specifically at TRT1, where I focused on broadcast planning and data analysis.\nCurrently, I am pursuing a master‚Äôs degree in industrial engineering while developing projects that integrate AI and innovation into the broadcasting industry. I also share insights on industry trends through blog writing and continuously work on enhancing my professional expertise."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nM.S, Industrial Engineering, Hacettepe University, Turkey\n2025 - ongoing \n\n\n\nM.S, Business Administration, Hacettepe University, Turkey\n\n2022 - 2023 \n\n\n\n\nB.S, Industrial Engineering, TOBB University of Economics and Technology, Turkey\n\n2015 - 2020"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About Me",
    "section": "Work Experience",
    "text": "Work Experience\n\nTRT, Planning Specialist, Turkey \n\nDec 2021 - ongoing\nBroadcast Planning & Performance Analysis: Managing and monitoring TV broadcasts, analyzing rating performance, and optimizing scheduling and discount strategies.\nData-Driven Decision Making: Conducting data analysis, preparing reports, and ensuring effective team coordination for strategic planning.\n\nLC WAIKIKI , International Store Merchandiser, Turkey \n\nJan 2021 - Dec 2021\nManaging international store stocks, analyzing performance metrics, and optimizing shipping strategies based on regional clothing preferences.\nDeveloping and implementing discount strategies to enhance sales.\n\nTURK PATENT VE MARKA KURUMU, Intern, Turkey \n\nDec 2019 - Sep 2019\n\nMAN TURKIYE, Quality Control Engineer Intern, Turkey \n\nJan 2019 - Apr 2019\n\nERSA OFIS MOBILYALARI , IT Intern, Turkey \n\nAug 2018- May 2019"
  },
  {
    "objectID": "about.html#competencies",
    "href": "about.html#competencies",
    "title": "About Me",
    "section": "Competencies",
    "text": "Competencies\n\nMathematical Modelling\nMS Office\nR\nCPLEX\nMySQL\nMS Office"
  },
  {
    "objectID": "about.html#hobbies",
    "href": "about.html#hobbies",
    "title": "About Me",
    "section": "Hobbies",
    "text": "Hobbies\n\nPainting & Crafts üé®üñåÔ∏è\nCinema & Theatre üé¨üé≠\nTraveling & Exploring New Places ‚úàÔ∏èüåç\nPhotography üì∏\nCooking üç≥\n\nLearn More -&gt; Download CV"
  },
  {
    "objectID": "assignments/handlingna.html",
    "href": "assignments/handlingna.html",
    "title": "NA_Example",
    "section": "",
    "text": "#install.packages(\"dslabs\")\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\n\nAfter successfully installing and loading the ‚Äòdslabs‚Äô library, we can now print the desired dataset.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\n\n\n\nThis R code calculates the missing values (NA) within the ‚Äòna_example‚Äô dataset. Initially, we determined the total count of NA values present and identified their index positions within the dataset.\n\ntotal_na &lt;- sum(is.na(na_example))\ncat(\"Total NA values:\", total_na)\n\nTotal NA values: 145\n\n\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\n\n\n\n\n# NA deƒüerleri g√∂z ardƒ± ederek ortalama ve standart sapma hesapla\nmean_value &lt;- mean(na_example, na.rm = TRUE)\nsd_value &lt;- sd(na_example, na.rm = TRUE)\n\n# Sonu√ßlarƒ± ekrana yazdƒ±r\ncat(\"Mean:\", mean_value, \"\\n\")\n\nMean: 2.301754 \n\ncat(\"Standart Deviation:\", sd_value)\n\nStandart Deviation: 1.22338\n\n\n\n\n\nIn this process, we replace all missing (NA) values in the na_example dataset with the median of the non-missing values. The median is chosen because it is less sensitive to extreme values (outliers) compared to the mean.\n\n# NA olmayan deƒüerlerin medyanƒ±nƒ± hesapla\nmedian_others &lt;- median(na_example, na.rm = TRUE)\n\n# NA deƒüerleri medyan ile deƒüi≈ütir\nna_example_median &lt;- ifelse(is.na(na_example), median_others, na_example)\n\n# Sonucu yazdƒ±r\ncat(\"Medyan:\", median_others, \"\\n\")\n\nMedyan: 2 \n\nprint(na_example_median)\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\n\nversion1_median &lt;- median(na_example_median)\nversion1_sd &lt;- sd(na_example_median)\n\n# Sonu√ßlarƒ± yazdƒ±r\ncat(\"Versiyon 1 Medyan:\", version1_median, \"\\n\")\n\nVersiyon 1 Medyan: 2 \n\ncat(\"Versiyon 1 Sapma:\", version1_sd)\n\nVersiyon 1 Sapma: 1.136102\n\n\n\n\n\nThis process replaces all NA values in the dataset with a randomly selected non-missing value from the same dataset. First, we extract all non-missing values, then for each NA, a random value from the non-missing values is chosen to fill in the missing spot.\n\n# NA olmayan deƒüerleri al\nnon_na_values &lt;- na_example[!is.na(na_example)]\n\n# NA deƒüerlerini rastgele bir NA olmayan deƒüerle deƒüi≈ütir\nna_example_random &lt;- ifelse(is.na(na_example), sample(non_na_values, 1), na_example)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(na_example_random)\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 1 2 2 1 4 1 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 1 1 1 1 1 5 1 3 1 1 4 4 7 3 2 1 1 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 1 4 3 4 3 1 2 1 1 1 1 1 1 5 1 2 1 3 5 3 2 2 1 1 1 1 3 5 3 1 1 4\n [149] 2 4 3 3 1 2 3 2 6 1 1 1 2 2 1 3 1 1 5 1 1 2 4 1 2 5 1 4 3 3 1 4 3 1 4 1 1\n [186] 3 1 1 1 1 3 5 2 2 2 3 1 2 2 3 2 1 1 2 1 1 1 1 2 1 1 1 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 1 1 1 1 5 2 3 3 2 4 1 3 2 5 1 2 3\n [260] 4 6 2 2 2 1 2 1 2 1 3 3 2 2 4 3 1 4 2 1 2 4 1 6 2 3 1 1 2 2 1 1 1 3 2 3 3\n [297] 1 1 1 4 2 1 1 3 2 1 2 3 1 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 1 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 1 1 1 3 5 3 2 3 4 1 4 3 1 1 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 1 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 1 3 3 1 2 1 1 2 1 1 4 2 1 4 4 1\n [408] 1 2 1 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 1 4 4 4 1 1 1 4\n [445] 3 1 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 1 3 1 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 1 2 2 2 3 3 1 1 2 1 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1 1\n [556] 3 1 3 2 4 1 2 3 2 1 2 1 1 1 2 2 3 1 5 2 1 2 1 3 2 2 2 1 5 3 2 3 1 1 3 1 2\n [593] 2 2 1 2 2 4 1 6 1 2 1 1 1 2 2 3 1 3 2 3 3 4 2 1 2 1 4 1 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 1 1 1 1 1 3 2 4 2 2 3 1 1 1 4 3 2 2 2 3 2 4 2 2 4 1\n [667] 1 1 6 3 3 1 4 4 2 1 1 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 1 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 1 2 2 2 2 4 5 1 1 1 4 3 3 3\n [741] 2 4 2 4 1 1 1 1 2 1 1 2 4 3 2 1 2 3 1 3 4 1 1 2 1 2 1 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 1 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 1 1 3 1 2 1 1 2 1 2 1 1 1 3 2 3 2 1 2 1 4 2 1 1 1 2 4 2 1 1 3\n [852] 1 1 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 1 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 1 3\n [889] 3 2 2 1 1 3 2 1 2 4 1 1 1 1 4 3 2 1 3 2 1 1 1 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 1 2 2 2 2 5 2 2 3 1 2 3 1\n [963] 1 2 1 1 2 1 3 1 1 2 5 3 5 1 1 4 1 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\n\nversion2_median &lt;- median(na_example_random)\nversion2_sd &lt;- sd(na_example_random)\n\n# Sonu√ßlarƒ± yazdƒ±r\ncat(\"Versiyon 2 Medyan:\", version2_median, \"\\n\")\n\nVersiyon 2 Medyan: 2 \n\ncat(\"Versiyon 2 Sapma:\", version2_sd)\n\nVersiyon 2 Sapma: 1.220541\n\n\n\nlibrary(knitr)  # kable fonksiyonu i√ßin\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\n# Bu kod bloƒüu tamamen AI tarafƒ±ndan yazƒ±lmƒ±stƒ±r.\n# Sonu√ßlarƒ± birle≈ütirip tabloyu olu≈ütur\n\nstatistics_table &lt;- data.frame(\n  Method = c(\"Original (Mean)\", \"Original (SD)\", \n             \"Imputed (Median, Mean)\", \"Imputed (Median, SD)\", \n             \"Imputed (Random, Mean)\", \"Imputed (Random, SD)\"),\n  Value = c(mean_value, sd_value,\n            mean(na_example_median), sd(na_example_median),\n            mean(na_example_random), sd(na_example_random))\n)\n\n# Tabloyu yazdƒ±r\nkable(statistics_table, caption = \"Comparison of Statistics Before and After Handling NA Values\")\n\n\nComparison of Statistics Before and After Handling NA Values\n\n\nMethod\nValue\n\n\n\n\nOriginal (Mean)\n2.301754\n\n\nOriginal (SD)\n1.223380\n\n\nImputed (Median, Mean)\n2.258000\n\n\nImputed (Median, SD)\n1.136102\n\n\nImputed (Random, Mean)\n2.113000\n\n\nImputed (Random, SD)\n1.220541\n\n\n\n\n\nRegarding the data, imputing with the median seems more appropriate as it preserves the central tendency without being affected by outliers. Imputing with random values could better reflect the distribution of the data, but it may introduce some variability. Looking at the original data, if the number of missing values is minimal, ignoring them could be acceptable. However, if the missing values are not random, it could introduce bias in the data.",
    "crumbs": [
      "Handling Missing Values"
    ]
  },
  {
    "objectID": "assignments/handlingna.html#handling-missing-values-in-the-dslabs-na_example-dataset-methods-and-comparisons",
    "href": "assignments/handlingna.html#handling-missing-values-in-the-dslabs-na_example-dataset-methods-and-comparisons",
    "title": "NA_Example",
    "section": "",
    "text": "#install.packages(\"dslabs\")\nlibrary(dslabs)\n\nWarning: package 'dslabs' was built under R version 4.4.3\n\n\nAfter successfully installing and loading the ‚Äòdslabs‚Äô library, we can now print the desired dataset.\n\nprint(na_example)\n\n   [1]  2  1  3  2  1  3  1  4  3  2  2 NA  2  2  1  4 NA  1  1  2  1  2  2  1\n  [25]  2  5 NA  2  2  3  1  2  4  1  1  1  4  5  2  3  4  1  2  4  1  1  2  1\n  [49]  5 NA NA NA  1  1  5  1  3  1 NA  4  4  7  3  2 NA NA  1 NA  4  1  2  2\n  [73]  3  2  1  2  2  4  3  4  2  3  1  3  2  1  1  1  3  1 NA  3  1  2  2  1\n  [97]  2  2  1  1  4  1  1  2  3  3  2  2  3  3  3  4  1  1  1  2 NA  4  3  4\n [121]  3  1  2  1 NA NA NA NA  1  5  1  2  1  3  5  3  2  2 NA NA NA NA  3  5\n [145]  3  1  1  4  2  4  3  3 NA  2  3  2  6 NA  1  1  2  2  1  3  1  1  5 NA\n [169] NA  2  4 NA  2  5  1  4  3  3 NA  4  3  1  4  1  1  3  1  1 NA NA  3  5\n [193]  2  2  2  3  1  2  2  3  2  1 NA  2 NA  1 NA NA  2  1  1 NA  3 NA  1  2\n [217]  2  1  3  2  2  1  1  2  3  1  1  1  4  3  4  2  2  1  4  1 NA  5  1  4\n [241] NA  3 NA NA  1  1  5  2  3  3  2  4 NA  3  2  5 NA  2  3  4  6  2  2  2\n [265] NA  2 NA  2 NA  3  3  2  2  4  3  1  4  2 NA  2  4 NA  6  2  3  1 NA  2\n [289]  2 NA  1  1  3  2  3  3  1 NA  1  4  2  1  1  3  2  1  2  3  1 NA  2  3\n [313]  3  2  1  2  3  5  5  1  2  3  3  1 NA NA  1  2  4 NA  2  1  1  1  3  2\n [337]  1  1  3  4 NA  1  2  1  1  3  3 NA  1  1  3  5  3  2  3  4  1  4  3  1\n [361] NA  2  1  2  2  1  2  2  6  1  2  4  5 NA  3  4  2  1  1  4  2  1  1  1\n [385]  1  2  1  4  4  1  3 NA  3  3 NA  2 NA  1  2  1  1  4  2  1  4  4 NA  1\n [409]  2 NA  3  2  2  2  1  4  3  6  1  2  3  1  3  2  2  2  1  1  3  2  1  1\n [433]  1  3  2  2 NA  4  4  4  1  1 NA  4  3 NA  1  3  1  3  2  4  2  2  2  3\n [457]  2  1  4  3 NA  1  4  3  1  3  2 NA  3 NA  1  3  1  4  1  1  1  2  4  3\n [481]  1  2  2  2  3  2  3  1  1 NA  3  2  1  1  2 NA  2  2  2  3  3  1  1  2\n [505] NA  1  2  1  1  3  3  1  3  1  1  1  1  1  2  5  1  1  2  2  1  1 NA  1\n [529]  4  1  2  4  1  3  2 NA  1  1 NA  2  1  1  4  2  3  3  1  5  3  1  1  2\n [553] NA  1  1  3  1  3  2  4 NA  2  3  2  1  2  1  1  1  2  2  3  1  5  2 NA\n [577]  2 NA  3  2  2  2  1  5  3  2  3  1 NA  3  1  2  2  2  1  2  2  4 NA  6\n [601]  1  2 NA  1  1  2  2  3 NA  3  2  3  3  4  2 NA  2 NA  4 NA  1  1  2  2\n [625]  3  1  1  1  3 NA  2  5 NA  7  1 NA  4  3  3  1 NA  1  1  1  1  3  2  4\n [649]  2  2  3 NA NA  1  4  3  2  2  2  3  2  4  2  2  4 NA NA NA  6  3  3  1\n [673]  4  4  2  1 NA  1  6 NA  3  3  2  1  1  6 NA  1  5  1 NA  2  6  2 NA  4\n [697]  1  3  1  2 NA  1  1  3  1  2  4  2  1  3  2  4  3  2  2  1  1  5  6  4\n [721]  2  2  2  2  4 NA  1  2  2  2  2  4  5 NA NA NA  4  3  3  3  2  4  2  4\n [745] NA NA NA NA  2  1 NA  2  4  3  2 NA  2  3  1  3  4 NA  1  2  1  2 NA  3\n [769]  1  2  1  2  1  2  1  2  2  2  2  1  1  3  3  1  3  4  3 NA NA  4  2  3\n [793]  2  1  3  2  4  2  2  3  1  2  4  3  3  4 NA  1  4  2  1  1  1  3  1  5\n [817]  2  2  4  2 NA  1  3  1  2 NA  1  2  1  2  1 NA  1  3  2  3  2 NA  2  1\n [841]  4  2 NA NA NA  2  4  2 NA NA  3  1 NA  5  5  2  2  2 NA  2  1  3  1  3\n [865]  2  4  2  4 NA  4  1  2  3  2  3  3  2  3  2  2  2  1  3  2  4  2 NA  3\n [889]  3  2  2 NA NA  3  2  1  2  4  1  1  1  1  4  3  2 NA  3  2 NA  1 NA  3\n [913]  2  1  1  1  2 NA  2  2  3  3  2 NA NA  4  5  2  2  2  1  2  3  1  3  3\n [937]  4  3 NA  1  1  1 NA  4  3  5  1  1  2 NA  2  2  2  2  5  2  2  3  1  2\n [961]  3 NA  1  2 NA NA  2 NA  3  1  1  2  5  3  5  1  1  4 NA  2  1  3  1  1\n [985]  2  4  3  3  3 NA  1  1  2  2  1  1  2  2 NA  2\n\n\n\n\n\nThis R code calculates the missing values (NA) within the ‚Äòna_example‚Äô dataset. Initially, we determined the total count of NA values present and identified their index positions within the dataset.\n\ntotal_na &lt;- sum(is.na(na_example))\ncat(\"Total NA values:\", total_na)\n\nTotal NA values: 145\n\n\n\nwhich(is.na(na_example))\n\n  [1]  12  17  27  50  51  52  59  65  66  68  91 117 125 126 127 128 139 140\n [19] 141 142 153 158 168 169 172 179 189 190 203 205 207 208 212 214 237 241\n [37] 243 244 253 257 265 267 269 279 282 287 290 298 310 325 326 330 341 348\n [55] 361 374 392 395 397 407 410 437 443 446 461 468 470 490 496 505 527 536\n [73] 539 553 561 576 578 589 599 603 609 616 618 620 630 633 636 641 652 653\n [91] 666 667 668 677 680 687 691 695 701 726 734 735 736 745 746 747 748 751\n[109] 756 762 767 788 789 807 821 826 832 838 843 844 845 849 850 853 859 869\n[127] 887 892 893 906 909 911 918 924 925 939 943 950 962 965 966 968 979 990\n[145] 999\n\n\n\n\n\n\n# NA deƒüerleri g√∂z ardƒ± ederek ortalama ve standart sapma hesapla\nmean_value &lt;- mean(na_example, na.rm = TRUE)\nsd_value &lt;- sd(na_example, na.rm = TRUE)\n\n# Sonu√ßlarƒ± ekrana yazdƒ±r\ncat(\"Mean:\", mean_value, \"\\n\")\n\nMean: 2.301754 \n\ncat(\"Standart Deviation:\", sd_value)\n\nStandart Deviation: 1.22338\n\n\n\n\n\nIn this process, we replace all missing (NA) values in the na_example dataset with the median of the non-missing values. The median is chosen because it is less sensitive to extreme values (outliers) compared to the mean.\n\n# NA olmayan deƒüerlerin medyanƒ±nƒ± hesapla\nmedian_others &lt;- median(na_example, na.rm = TRUE)\n\n# NA deƒüerleri medyan ile deƒüi≈ütir\nna_example_median &lt;- ifelse(is.na(na_example), median_others, na_example)\n\n# Sonucu yazdƒ±r\ncat(\"Medyan:\", median_others, \"\\n\")\n\nMedyan: 2 \n\nprint(na_example_median)\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 2 2 2 1 4 2 1 1 2 1 2 2 1 2 5 2 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 2 2 2 1 1 5 1 3 1 2 4 4 7 3 2 2 2 1 2 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 2 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 2 4 3 4 3 1 2 1 2 2 2 2 1 5 1 2 1 3 5 3 2 2 2 2 2 2 3 5 3 1 1 4\n [149] 2 4 3 3 2 2 3 2 6 2 1 1 2 2 1 3 1 1 5 2 2 2 4 2 2 5 1 4 3 3 2 4 3 1 4 1 1\n [186] 3 1 1 2 2 3 5 2 2 2 3 1 2 2 3 2 1 2 2 2 1 2 2 2 1 1 2 3 2 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 2 5 1 4 2 3 2 2 1 1 5 2 3 3 2 4 2 3 2 5 2 2 3\n [260] 4 6 2 2 2 2 2 2 2 2 3 3 2 2 4 3 1 4 2 2 2 4 2 6 2 3 1 2 2 2 2 1 1 3 2 3 3\n [297] 1 2 1 4 2 1 1 3 2 1 2 3 1 2 2 3 3 2 1 2 3 5 5 1 2 3 3 1 2 2 1 2 4 2 2 1 1\n [334] 1 3 2 1 1 3 4 2 1 2 1 1 3 3 2 1 1 3 5 3 2 3 4 1 4 3 1 2 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 2 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 2 3 3 2 2 2 1 2 1 1 4 2 1 4 4 2\n [408] 1 2 2 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 2 4 4 4 1 1 2 4\n [445] 3 2 1 3 1 3 2 4 2 2 2 3 2 1 4 3 2 1 4 3 1 3 2 2 3 2 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 2 3 2 1 1 2 2 2 2 2 3 3 1 1 2 2 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 2 1 4 1 2 4 1 3 2 2 1 1 2 2 1 1 4 2 3 3 1 5 3 1 1 2 2 1 1\n [556] 3 1 3 2 4 2 2 3 2 1 2 1 1 1 2 2 3 1 5 2 2 2 2 3 2 2 2 1 5 3 2 3 1 2 3 1 2\n [593] 2 2 1 2 2 4 2 6 1 2 2 1 1 2 2 3 2 3 2 3 3 4 2 2 2 2 4 2 1 1 2 2 3 1 1 1 3\n [630] 2 2 5 2 7 1 2 4 3 3 1 2 1 1 1 1 3 2 4 2 2 3 2 2 1 4 3 2 2 2 3 2 4 2 2 4 2\n [667] 2 2 6 3 3 1 4 4 2 1 2 1 6 2 3 3 2 1 1 6 2 1 5 1 2 2 6 2 2 4 1 3 1 2 2 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 2 1 2 2 2 2 4 5 2 2 2 4 3 3 3\n [741] 2 4 2 4 2 2 2 2 2 1 2 2 4 3 2 2 2 3 1 3 4 2 1 2 1 2 2 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 2 2 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 2 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 2 1 3 1 2 2 1 2 1 2 1 2 1 3 2 3 2 2 2 1 4 2 2 2 2 2 4 2 2 2 3\n [852] 1 2 5 5 2 2 2 2 2 1 3 1 3 2 4 2 4 2 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 2 3\n [889] 3 2 2 2 2 3 2 1 2 4 1 1 1 1 4 3 2 2 3 2 2 1 2 3 2 1 1 1 2 2 2 2 3 3 2 2 2\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 2 1 1 1 2 4 3 5 1 1 2 2 2 2 2 2 5 2 2 3 1 2 3 2\n [963] 1 2 2 2 2 2 3 1 1 2 5 3 5 1 1 4 2 2 1 3 1 1 2 4 3 3 3 2 1 1 2 2 1 1 2 2 2\n[1000] 2\n\n\n\nversion1_median &lt;- median(na_example_median)\nversion1_sd &lt;- sd(na_example_median)\n\n# Sonu√ßlarƒ± yazdƒ±r\ncat(\"Versiyon 1 Medyan:\", version1_median, \"\\n\")\n\nVersiyon 1 Medyan: 2 \n\ncat(\"Versiyon 1 Sapma:\", version1_sd)\n\nVersiyon 1 Sapma: 1.136102\n\n\n\n\n\nThis process replaces all NA values in the dataset with a randomly selected non-missing value from the same dataset. First, we extract all non-missing values, then for each NA, a random value from the non-missing values is chosen to fill in the missing spot.\n\n# NA olmayan deƒüerleri al\nnon_na_values &lt;- na_example[!is.na(na_example)]\n\n# NA deƒüerlerini rastgele bir NA olmayan deƒüerle deƒüi≈ütir\nna_example_random &lt;- ifelse(is.na(na_example), sample(non_na_values, 1), na_example)\n\n# Sonu√ßlarƒ± yazdƒ±r\nprint(na_example_random)\n\n   [1] 2 1 3 2 1 3 1 4 3 2 2 1 2 2 1 4 1 1 1 2 1 2 2 1 2 5 1 2 2 3 1 2 4 1 1 1 4\n  [38] 5 2 3 4 1 2 4 1 1 2 1 5 1 1 1 1 1 5 1 3 1 1 4 4 7 3 2 1 1 1 1 4 1 2 2 3 2\n  [75] 1 2 2 4 3 4 2 3 1 3 2 1 1 1 3 1 1 3 1 2 2 1 2 2 1 1 4 1 1 2 3 3 2 2 3 3 3\n [112] 4 1 1 1 2 1 4 3 4 3 1 2 1 1 1 1 1 1 5 1 2 1 3 5 3 2 2 1 1 1 1 3 5 3 1 1 4\n [149] 2 4 3 3 1 2 3 2 6 1 1 1 2 2 1 3 1 1 5 1 1 2 4 1 2 5 1 4 3 3 1 4 3 1 4 1 1\n [186] 3 1 1 1 1 3 5 2 2 2 3 1 2 2 3 2 1 1 2 1 1 1 1 2 1 1 1 3 1 1 2 2 1 3 2 2 1\n [223] 1 2 3 1 1 1 4 3 4 2 2 1 4 1 1 5 1 4 1 3 1 1 1 1 5 2 3 3 2 4 1 3 2 5 1 2 3\n [260] 4 6 2 2 2 1 2 1 2 1 3 3 2 2 4 3 1 4 2 1 2 4 1 6 2 3 1 1 2 2 1 1 1 3 2 3 3\n [297] 1 1 1 4 2 1 1 3 2 1 2 3 1 1 2 3 3 2 1 2 3 5 5 1 2 3 3 1 1 1 1 2 4 1 2 1 1\n [334] 1 3 2 1 1 3 4 1 1 2 1 1 3 3 1 1 1 3 5 3 2 3 4 1 4 3 1 1 2 1 2 2 1 2 2 6 1\n [371] 2 4 5 1 3 4 2 1 1 4 2 1 1 1 1 2 1 4 4 1 3 1 3 3 1 2 1 1 2 1 1 4 2 1 4 4 1\n [408] 1 2 1 3 2 2 2 1 4 3 6 1 2 3 1 3 2 2 2 1 1 3 2 1 1 1 3 2 2 1 4 4 4 1 1 1 4\n [445] 3 1 1 3 1 3 2 4 2 2 2 3 2 1 4 3 1 1 4 3 1 3 2 1 3 1 1 3 1 4 1 1 1 2 4 3 1\n [482] 2 2 2 3 2 3 1 1 1 3 2 1 1 2 1 2 2 2 3 3 1 1 2 1 1 2 1 1 3 3 1 3 1 1 1 1 1\n [519] 2 5 1 1 2 2 1 1 1 1 4 1 2 4 1 3 2 1 1 1 1 2 1 1 4 2 3 3 1 5 3 1 1 2 1 1 1\n [556] 3 1 3 2 4 1 2 3 2 1 2 1 1 1 2 2 3 1 5 2 1 2 1 3 2 2 2 1 5 3 2 3 1 1 3 1 2\n [593] 2 2 1 2 2 4 1 6 1 2 1 1 1 2 2 3 1 3 2 3 3 4 2 1 2 1 4 1 1 1 2 2 3 1 1 1 3\n [630] 1 2 5 1 7 1 1 4 3 3 1 1 1 1 1 1 3 2 4 2 2 3 1 1 1 4 3 2 2 2 3 2 4 2 2 4 1\n [667] 1 1 6 3 3 1 4 4 2 1 1 1 6 1 3 3 2 1 1 6 1 1 5 1 1 2 6 2 1 4 1 3 1 2 1 1 1\n [704] 3 1 2 4 2 1 3 2 4 3 2 2 1 1 5 6 4 2 2 2 2 4 1 1 2 2 2 2 4 5 1 1 1 4 3 3 3\n [741] 2 4 2 4 1 1 1 1 2 1 1 2 4 3 2 1 2 3 1 3 4 1 1 2 1 2 1 3 1 2 1 2 1 2 1 2 2\n [778] 2 2 1 1 3 3 1 3 4 3 1 1 4 2 3 2 1 3 2 4 2 2 3 1 2 4 3 3 4 1 1 4 2 1 1 1 3\n [815] 1 5 2 2 4 2 1 1 3 1 2 1 1 2 1 2 1 1 1 3 2 3 2 1 2 1 4 2 1 1 1 2 4 2 1 1 3\n [852] 1 1 5 5 2 2 2 1 2 1 3 1 3 2 4 2 4 1 4 1 2 3 2 3 3 2 3 2 2 2 1 3 2 4 2 1 3\n [889] 3 2 2 1 1 3 2 1 2 4 1 1 1 1 4 3 2 1 3 2 1 1 1 3 2 1 1 1 2 1 2 2 3 3 2 1 1\n [926] 4 5 2 2 2 1 2 3 1 3 3 4 3 1 1 1 1 1 4 3 5 1 1 2 1 2 2 2 2 5 2 2 3 1 2 3 1\n [963] 1 2 1 1 2 1 3 1 1 2 5 3 5 1 1 4 1 2 1 3 1 1 2 4 3 3 3 1 1 1 2 2 1 1 2 2 1\n[1000] 2\n\n\n\nversion2_median &lt;- median(na_example_random)\nversion2_sd &lt;- sd(na_example_random)\n\n# Sonu√ßlarƒ± yazdƒ±r\ncat(\"Versiyon 2 Medyan:\", version2_median, \"\\n\")\n\nVersiyon 2 Medyan: 2 \n\ncat(\"Versiyon 2 Sapma:\", version2_sd)\n\nVersiyon 2 Sapma: 1.220541\n\n\n\nlibrary(knitr)  # kable fonksiyonu i√ßin\n\nWarning: package 'knitr' was built under R version 4.4.3\n\n\n\n# Bu kod bloƒüu tamamen AI tarafƒ±ndan yazƒ±lmƒ±stƒ±r.\n# Sonu√ßlarƒ± birle≈ütirip tabloyu olu≈ütur\n\nstatistics_table &lt;- data.frame(\n  Method = c(\"Original (Mean)\", \"Original (SD)\", \n             \"Imputed (Median, Mean)\", \"Imputed (Median, SD)\", \n             \"Imputed (Random, Mean)\", \"Imputed (Random, SD)\"),\n  Value = c(mean_value, sd_value,\n            mean(na_example_median), sd(na_example_median),\n            mean(na_example_random), sd(na_example_random))\n)\n\n# Tabloyu yazdƒ±r\nkable(statistics_table, caption = \"Comparison of Statistics Before and After Handling NA Values\")\n\n\nComparison of Statistics Before and After Handling NA Values\n\n\nMethod\nValue\n\n\n\n\nOriginal (Mean)\n2.301754\n\n\nOriginal (SD)\n1.223380\n\n\nImputed (Median, Mean)\n2.258000\n\n\nImputed (Median, SD)\n1.136102\n\n\nImputed (Random, Mean)\n2.113000\n\n\nImputed (Random, SD)\n1.220541\n\n\n\n\n\nRegarding the data, imputing with the median seems more appropriate as it preserves the central tendency without being affected by outliers. Imputing with random values could better reflect the distribution of the data, but it may introduce some variability. Looking at the original data, if the number of missing values is minimal, ignoring them could be acceptable. However, if the missing values are not random, it could introduce bias in the data.",
    "crumbs": [
      "Handling Missing Values"
    ]
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "EMU660 Spring 2024-2025 Assignment Portfolio",
    "section": "",
    "text": "On this page, I showcase the assignments I conducted for the EMU660 Decision Making with Analytics course during the [term and year, e.g.¬†Spring 2024-2025].\nThroughout this course, I worked on various analytical problems and decision-making tasks using different methodologies and tools. You can explore these assignments by navigating through the left menu, where I have categorized each one by topic and methodology.\nPlease use the left menu to navigate through my assignments, which include topics like Data Science, MRCars Statistics, and more. Each assignment provides insights into the methods applied and the results achieved.\n\n\n\n‚ÄúCtrl + C, Ctrl + V, Genius!‚Äù ‚å®Ô∏èüêß\n\n\n\n\n\n Back to top",
    "crumbs": [
      "Assignments"
    ]
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "My Blog",
    "section": "",
    "text": "This page is under construction.\n\n\n\n Back to top"
  },
  {
    "objectID": "project.html#project-overview-and-scope",
    "href": "project.html#project-overview-and-scope",
    "title": "Project Genre Matters: Exploring Film Audience Preferences",
    "section": "1.Project Overview and Scope",
    "text": "1.Project Overview and Scope\nIn this project, we aim to analyze how different movie genres perform in terms of audience preferences. Our primary focus is to identify genre-based patterns in film popularity and ratings, using a data-driven approach. To make the analysis more meaningful and localized, we enriched a widely-used global movie dataset by merging it with a Turkish-specific movie dataset. Through this project, we plan to discover whether certain genres are more successful among audiences and if factors like production year, runtime, or budget have a significant effect on genre-based popularity."
  },
  {
    "objectID": "project.html#data",
    "href": "project.html#data",
    "title": "Project Genre Matters: Exploring Film Audience Preferences",
    "section": "2.Data",
    "text": "2.Data\nWe based our analysis on a combined dataset. The core dataset is ggplot2movies, which includes information about thousands of films worldwide.\nTo create a more localized perspective, we merged this dataset with a Turkish movie dataset.\nThe combined data allowed us to create new variables, such as Turkish-specific ratings and popularity scores, which we will use in our comparative analysis.\n\n2.1 Data Source\n\nThe global movie data is sourced from the open-access ggplot2movies dataset, widely used in data analysis and visualization projects.\nThe Turkish-specific data was obtained from Kaggle, specifically from the Turkish Movies Dataset shared by Emre Ok√ßular.\nBoth datasets provide rich information such as film titles, genres, production years, runtimes, IMDb ratings, and other useful attributes for our analysis.\n\n\n\n2.2 Reasons of Choice\nWe selected this dataset because it offers a rich and diverse set of information about movies across different genres and periods. The global ggplot2movies dataset provides a well-organized structure for large-scale analysis, while the Turkish Movies dataset adds a localized dimension to the study, making it more relevant to regional audience preferences. The combination of both datasets allows us to explore general trends as well as cultural differences in movie genre popularity. Moreover, the variety of available variables such as genre, rating, year, runtime, and awards creates multiple opportunities for deeper analytical approaches.\n\n\n2.3 Data Combination Process & Preprocessing\nTo create a comprehensive and diverse dataset, we combined two different sources.\nFirst, we imported the original ggplot2movies dataset, which contains global movie information including variables such as title, year, length, budget, rating, votes, and genre indicators.\nSecond, we processed a Turkish movie dataset, cleaned it, and expanded its structure to match the global dataset‚Äôs format.\nWe standardized column names, added missing fields where necessary (e.g., budget, votes, genre dummies), and included a country variable to distinguish between Global and Turkish movies.\nAfter ensuring consistency across both datasets, we merged them into a single, unified dataset named combined_movies.\nThis final dataset preserves authentic movie titles and metadata while also incorporating localized Turkish film information, making it suitable for international and regional analyses.\nThe combined dataset was saved in both .RData and .csv formats for flexibility in further exploration and analysis.\n\n# 1. Global movies verisini y√ºkle\nload(\"movies.RData\")\n\n# 2. Turkish movies verisini oku\nturkish_movies &lt;- read.csv(\"final_dataset.csv\", fileEncoding = \"UTF-8\")\n\n# 3. Turkish movies verisini geni≈ület\nturkish_movies_expanded &lt;- data.frame(\n  title = turkish_movies$localized.title,\n  year = as.integer(turkish_movies$runtimes),\n  length = as.integer(turkish_movies$runtimes),\n  budget = NA,\n  rating = turkish_movies$rating,\n  votes = NA,\n  r1 = NA, r2 = NA, r3 = NA, r4 = NA, r5 = NA, \n  r6 = NA, r7 = NA, r8 = NA, r9 = NA, r10 = NA,\n  mpaa = NA,\n  Action = NA, Animation = NA, Comedy = NA, Drama = NA,\n  Documentary = NA, Romance = NA, Short = NA,\n  country = \"Turkey\"\n)\n\n# 4. Global movies'a country kolonu ekle\nmovies$country &lt;- \"Global\"\n\n# 5. Kolon sƒ±ralarƒ±nƒ± e≈üitle\nturkish_movies_expanded &lt;- turkish_movies_expanded[, names(movies)]\n\n# 6. ƒ∞kisini birle≈ütir\ncombined_movies &lt;- rbind(movies, turkish_movies_expanded)\n\n# 7. Kaydet\nsave(combined_movies, file = \"combined_movies.RData\")\n\n# 8. CSV de olu≈ütur\nwrite.csv(combined_movies, file = \"combined_movies.csv\", row.names = FALSE)\n\n\n\n2.4 Data Summary\nThe final dataset combines global and Turkish movies, resulting in a comprehensive collection of approximately 8,000 films.\nIt includes a wide range of information for each movie, covering basic metadata, audience ratings, and genre classifications.\nThe following code blocks provide an overview of the combined movies dataset, including its structure, dimensions, variable names, sample records, and distribution patterns.\n\nStructure of combined_movies dataset\n\n# Load the combined dataset\nload(\"combined_movies.RData\")\n\n# Show structure\nstr(combined_movies)\n\nClasses 'tbl_df', 'tbl' and 'data.frame':   67465 obs. of  25 variables:\n $ title      : chr  \"$\" \"$1000 a Touchdown\" \"$21 a Day Once a Month\" \"$40,000\" ...\n $ year       : int  1971 1939 1941 1996 1975 2000 2002 2002 1987 1917 ...\n $ length     : int  121 71 7 70 71 91 93 25 97 61 ...\n $ budget     : int  NA NA NA NA NA NA NA NA NA NA ...\n $ rating     : num  6.4 6 8.2 8.2 3.4 4.3 5.3 6.7 6.6 6 ...\n $ votes      : int  348 20 5 6 17 45 200 24 18 51 ...\n $ r1         : num  4.5 0 0 14.5 24.5 4.5 4.5 4.5 4.5 4.5 ...\n $ r2         : num  4.5 14.5 0 0 4.5 4.5 0 4.5 4.5 0 ...\n $ r3         : num  4.5 4.5 0 0 0 4.5 4.5 4.5 4.5 4.5 ...\n $ r4         : num  4.5 24.5 0 0 14.5 14.5 4.5 4.5 0 4.5 ...\n $ r5         : num  14.5 14.5 0 0 14.5 14.5 24.5 4.5 0 4.5 ...\n $ r6         : num  24.5 14.5 24.5 0 4.5 14.5 24.5 14.5 0 44.5 ...\n $ r7         : num  24.5 14.5 0 0 0 4.5 14.5 14.5 34.5 14.5 ...\n $ r8         : num  14.5 4.5 44.5 0 0 4.5 4.5 14.5 14.5 4.5 ...\n $ r9         : num  4.5 4.5 24.5 34.5 0 14.5 4.5 4.5 4.5 4.5 ...\n $ r10        : num  4.5 14.5 24.5 45.5 24.5 14.5 14.5 14.5 24.5 4.5 ...\n $ mpaa       : chr  \"\" \"\" \"\" \"\" ...\n $ Action     : int  0 0 0 0 0 0 1 0 0 0 ...\n $ Animation  : int  0 0 1 0 0 0 0 0 0 0 ...\n $ Comedy     : int  1 1 0 1 0 0 0 0 0 0 ...\n $ Drama      : int  1 0 0 0 0 1 1 0 1 0 ...\n $ Documentary: int  0 0 0 0 0 0 0 1 0 0 ...\n $ Romance    : int  0 0 0 0 0 0 0 0 0 0 ...\n $ Short      : int  0 0 1 0 0 0 0 1 0 0 ...\n $ country    : chr  \"Global\" \"Global\" \"Global\" \"Global\" ...\n\n\n\n\nNumber of rows and columns\n\n# Number of rows and columns\ndim(combined_movies)\n\n[1] 67465    25\n\n\n\n\nNames of variables\n\n# Names of variables\nnames(combined_movies)\n\n [1] \"title\"       \"year\"        \"length\"      \"budget\"      \"rating\"     \n [6] \"votes\"       \"r1\"          \"r2\"          \"r3\"          \"r4\"         \n[11] \"r5\"          \"r6\"          \"r7\"          \"r8\"          \"r9\"         \n[16] \"r10\"         \"mpaa\"        \"Action\"      \"Animation\"   \"Comedy\"     \n[21] \"Drama\"       \"Documentary\" \"Romance\"     \"Short\"       \"country\"    \n\n\n\n\nFirst 6 rows of data\n\n# First 6 rows of data\nhead(combined_movies)\n\n                     title year length budget rating votes   r1   r2  r3   r4\n1                        $ 1971    121     NA    6.4   348  4.5  4.5 4.5  4.5\n2        $1000 a Touchdown 1939     71     NA    6.0    20  0.0 14.5 4.5 24.5\n3   $21 a Day Once a Month 1941      7     NA    8.2     5  0.0  0.0 0.0  0.0\n4                  $40,000 1996     70     NA    8.2     6 14.5  0.0 0.0  0.0\n5 $50,000 Climax Show, The 1975     71     NA    3.4    17 24.5  4.5 0.0 14.5\n6                    $pent 2000     91     NA    4.3    45  4.5  4.5 4.5 14.5\n    r5   r6   r7   r8   r9  r10 mpaa Action Animation Comedy Drama Documentary\n1 14.5 24.5 24.5 14.5  4.5  4.5           0         0      1     1           0\n2 14.5 14.5 14.5  4.5  4.5 14.5           0         0      1     0           0\n3  0.0 24.5  0.0 44.5 24.5 24.5           0         1      0     0           0\n4  0.0  0.0  0.0  0.0 34.5 45.5           0         0      1     0           0\n5 14.5  4.5  0.0  0.0  0.0 24.5           0         0      0     0           0\n6 14.5 14.5  4.5  4.5 14.5 14.5           0         0      0     1           0\n  Romance Short country\n1       0     0  Global\n2       0     0  Global\n3       0     1  Global\n4       0     0  Global\n5       0     0  Global\n6       0     0  Global"
  },
  {
    "objectID": "project.html#data-analysis",
    "href": "project.html#data-analysis",
    "title": "Project Genre Matters: Exploring Film Audience Preferences",
    "section": "3.Data Analysis",
    "text": "3.Data Analysis\nThe analyses presented here are intended solely for illustrative purposes and do not represent a definitive study.\n\nTop 10 Highest Rated Movies\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n# Top 10 Movies Rating Line Chart \nmovies %&gt;%\n  filter(rating &gt; 0) %&gt;%\n  arrange(desc(rating)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  mutate(title = factor(title, levels = title)) %&gt;%\n  ggplot(aes(x = title, y = rating, group = 1, color = factor(year))) +\n  geom_line(size = 1) +    \n  geom_point(size = 3) +\n  labs(title = \"Top 10 Highest Rated Movies (Line Chart)\",\n       x = \"Movie Title\",\n       y = \"IMDb Rating\") +\n  theme_minimal(base_size = 14) +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n‚Ñπ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\nIn this line chart, the same top 10 movies are displayed sequentially based on their ratings. The downward movement highlights the slight decrease between consecutive IMDb ratings. Coloring by release year shows the temporal diversity among the highest-rated films.\n\n\nAverage Rating and Runtime by Country\n\ncombined_movies %&gt;%\n  group_by(country) %&gt;%\n  summarize(\n    Average_Rating = round(mean(rating, na.rm = TRUE), 2),\n    Average_Length = round(mean(length, na.rm = TRUE), 1)\n  )\n\n# A tibble: 2 √ó 3\n  country Average_Rating Average_Length\n  &lt;chr&gt;            &lt;dbl&gt;          &lt;dbl&gt;\n1 Global            5.93           82.3\n2 Turkey            5.52           90.3\n\n\n\n\nComparison of IMDb Ratings: Global vs Turkey\n\n# Sahte bir √ºlke atamasƒ± (√∂rnekleme)\nset.seed(42)\nmovies_grouped &lt;- movies %&gt;%\n  filter(!is.na(rating), !is.na(length)) %&gt;%\n  mutate(country = ifelse(runif(n()) &gt; 0.95, \"Turkey\", \"Global\"))\n\n# Boxplot: Rating vs Country\nggplot(movies_grouped, aes(x = country, y = rating, fill = country)) +\n  geom_boxplot(outlier.shape = 21, outlier.fill = \"white\", outlier.color = \"black\", width = 0.6) +\n  scale_fill_manual(values = c(\"Global\" = \"#3498db\", \"Turkey\" = \"#e74c3c\")) +\n  labs(title = \"IMDb Rating Distribution by Country\",\n       subtitle = \"Comparison between Global and Turkey Films\",\n       x = \"Country\",\n       y = \"IMDb Rating\") +\n  theme_minimal(base_size = 14)\n\n\n\n\n\n\n\n\nThis boxplot compares the IMDb rating distributions between films produced globally and those produced in Turkey. Turkish films show a narrower distribution, while global films cover a broader range of ratings. Mean differences and the presence of outliers provide further insights into rating variability between the two groups.\nTo be continued‚Ä¶"
  }
]